{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_attention_pointer.py\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "#\n",
    "# -----------------------------\n",
    "# Data: \"copy at index\" task\n",
    "# -----------------------------\n",
    "def make_batch(batch_size, seq_len, vocab_tokens, device):\n",
    "    \"\"\"\n",
    "    Sequence: [x0 x1 ... x_{L-1} Qk]\n",
    "    Target: x_k\n",
    "    \"\"\"\n",
    "    # tokens are 0..(V-1), query tokens are V..(V+L-1) where query V+k means \"ask for position k\"\n",
    "    V = vocab_tokens\n",
    "    L = seq_len\n",
    "\n",
    "    x = torch.randint(0, V, (batch_size, L), device=device)\n",
    "    k = torch.randint(0, L, (batch_size,), device=device)\n",
    "    q = (V + k).unsqueeze(1)  # [B, 1]\n",
    "    inp = torch.cat([x, q], dim=1)  # [B, L+1]\n",
    "    tgt = x[torch.arange(batch_size, device=device), k]  # [B]\n",
    "    return inp, tgt, k\n",
    "\n",
    "def encode_example(x, k, V, L, device):\n",
    "    assert len(x) == L\n",
    "    inp = torch.tensor(x + [V + k], device=device).unsqueeze(0)  # [1, L+1]\n",
    "    tgt = torch.tensor([x[k]], device=device)                    # [1]\n",
    "    return inp, tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model: 1-head self-attention\n",
    "# -----------------------------\n",
    "class OneHeadPointer(nn.Module):\n",
    "    def __init__(self, vocab_tokens, seq_len, d_model=32):\n",
    "        super().__init__()\n",
    "        self.V = vocab_tokens\n",
    "        self.L = seq_len\n",
    "        self.T = seq_len + 1  # incl. query position\n",
    "\n",
    "        # Total vocab includes query tokens Q0..Q_{L-1}\n",
    "        self.emb = nn.Embedding(vocab_tokens + seq_len, d_model)\n",
    "\n",
    "        # learned positional embeddings (keep it simple)\n",
    "        self.pos = nn.Parameter(torch.randn(self.T, d_model) * 0.02)\n",
    "\n",
    "        # single-head attention projections\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # classify only from the *query position* output to original token vocab\n",
    "        self.out = nn.Linear(d_model, vocab_tokens)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input: [B, T]\n",
    "        returns:\n",
    "          logits: [B, V]\n",
    "          attn:   [B, T, T]  (attention weights for every position -> every position)\n",
    "        \"\"\"\n",
    "        B, T = input.shape\n",
    "        h = self.emb(input) + self.pos.unsqueeze(0)  # [B, T, D]\n",
    "\n",
    "        Q = self.Wq(h)  # [B, T, D]\n",
    "        K = self.Wk(h)  # [B, T, D]\n",
    "        V = self.Wv(h)  # [B, T, D]\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(h.size(-1))  # [B, T, T]\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        z = attn @ V  # [B, T, D]\n",
    "\n",
    "        # take last position (query) and predict token\n",
    "        zq = z[:, -1, :]  # [B, D]\n",
    "        logits = self.out(zq)  # [B, V]\n",
    "        return logits, attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tokens = 12   # tokens: 0..11\n",
    "seq_len = 8         # positions 0..7, query tokens Q0..Q7\n",
    "d_model = 48\n",
    "batch_size = 256\n",
    "steps = 200\n",
    "lr = 3e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e873c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_inp, fixed_tgt, fixed_k = make_batch(1, seq_len, vocab_tokens, device)\n",
    "\n",
    "print(f\"input tokens: {fixed_inp[0].tolist()} (query token {fixed_inp[0, -1].item()} asks for position {fixed_k.item()})\")\n",
    "print(f\"target token: {fixed_tgt.item()} (token at position {fixed_k.item()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84188b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_attention(query_attn, fixed_inp_cpu, vocab_tokens, seq_len):\n",
    "    T = seq_len + 1\n",
    "    tokens = fixed_inp_cpu[0].tolist()\n",
    "    # label tokens: regular tokens as t#, query token as Qk\n",
    "    labels = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if i < seq_len:\n",
    "            labels.append(f\"x{i}:{tok}\")\n",
    "        else:\n",
    "            labels.append(f\"Q{tok - vocab_tokens}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 2.2))\n",
    "    plt.imshow(query_attn.unsqueeze(0), aspect=\"auto\")\n",
    "    plt.yticks([0], [\"query\"])\n",
    "    plt.xticks(range(T), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(\"Attention weights from query position\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226827bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train + visualize attention\n",
    "# -----------------------------\n",
    "model = OneHeadPointer(vocab_tokens, seq_len, d_model=d_model).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# a fixed example we will visualize during training\n",
    "fixed_inp, fixed_tgt, fixed_k = make_batch(1, seq_len, vocab_tokens, device)\n",
    "fixed_inp_cpu = fixed_inp.detach().cpu()\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    inp, tgt, _ = make_batch(batch_size, seq_len, vocab_tokens, device)\n",
    "    logits, _ = model(inp)\n",
    "    loss = F.cross_entropy(logits, tgt)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 20 == 0 or step == 1:\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            acc = (pred == tgt).float().mean().item()\n",
    "\n",
    "            flogits, fattn = model(fixed_inp)\n",
    "            fpred = flogits.argmax(dim=-1).item()\n",
    "            fk = fixed_k.item()\n",
    "            ftrue = fixed_tgt.item()\n",
    "\n",
    "            print(f\"step {step:4d} | loss {loss.item():.4f} | acc {acc*100:5.1f}% \"\n",
    "                    f\"| fixed query k={fk} true={ftrue} pred={fpred}\")\n",
    "        if step < 100:\n",
    "            visualise_attention(fattn[0, -1].detach().cpu(), fixed_inp_cpu, vocab_tokens, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15071cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(seq_len):\n",
    "    x = [3, 7, 1, 11, 0, 0, 9, 2]  # length L=8\n",
    "#k = 3\n",
    "    fixed_inp, fixed_tgt = encode_example(x, k, vocab_tokens, seq_len, device)\n",
    "\n",
    "    visualise_attention(model(fixed_inp)[1][0, -1].detach().cpu(), fixed_inp_cpu, vocab_tokens, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72df8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9c7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
