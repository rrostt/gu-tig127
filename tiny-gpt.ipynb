{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 1+2=3\n",
    "# 1+4=5\n",
    "# -----------------------\n",
    "# Data: a+b=(a+b)%10\n",
    "# -----------------------\n",
    "itos = list(\"0123456789+=\")          # index -> char\n",
    "stoi = {c:i for i,c in enumerate(itos)}\n",
    "V = len(itos)\n",
    "\n",
    "def make_batch(batch_size, device):\n",
    "    # sequence length 5: d + d = d\n",
    "    a = torch.randint(0, 10, (batch_size,), device=device)\n",
    "    b = torch.randint(0, 10, (batch_size,), device=device)\n",
    "    c = (a + b) % 10\n",
    "\n",
    "    # tokens: [a, '+', b, '=', c]\n",
    "    x = torch.stack([\n",
    "        a, \n",
    "        torch.full_like(a, stoi['+']),\n",
    "        b,\n",
    "        torch.full_like(a, stoi['=']),\n",
    "        c\n",
    "    ], dim=1)  # [B, 5]\n",
    "\n",
    "    # next-token prediction: input is first 4 tokens, target is next 4 tokens\n",
    "    # e.g. input:  \"3+7=\"  target: \"+7=0\" (shifted by one)\n",
    "    inp = x[:, :-1]   # [B, 4]\n",
    "    tgt = x[:, 1:]    # [B, 4]\n",
    "    return inp, tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Tiny GPT (decoder-only)\n",
    "# -----------------------\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=2, n_layers=2, max_len=4):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Parameter(torch.randn(max_len, d_model) * 0.02)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            activation=\"gelu\", batch_first=True\n",
    "        )\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # causal mask for length max_len\n",
    "        mask = torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        self.register_buffer(\"causal_mask\", mask)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: [B, T]\n",
    "        B, T = inp.shape\n",
    "        h = self.tok(inp) + self.pos[:T].unsqueeze(0)\n",
    "        h = self.tr(h, mask=self.causal_mask[:T, :T])\n",
    "        logits = self.lm_head(h)  # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "model = TinyGPT(vocab_size=V, d_model=64, n_heads=2, n_layers=2, max_len=4).to(device)\n",
    "inp, _ = make_batch(2, device)\n",
    "out = model(inp)  # test forward pass\n",
    "out[0,-1].detach().cpu().numpy()  # logits for last token of first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, prompt_tokens, steps=1):\n",
    "    # prompt_tokens: list[int], length <= 4\n",
    "    # if prompt is empty, pick random number as first token\n",
    "    if len(prompt_tokens) == 0:\n",
    "        prompt_tokens = [random.randint(0, 9)]\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.tensor(prompt_tokens, device=device).unsqueeze(0)  # [1, T]\n",
    "    for _ in range(steps):\n",
    "        logits = model(x)                 # [1, T, V]\n",
    "        next_logits = logits[0, -1]       # [V]\n",
    "        nxt = torch.argmax(next_logits).item()\n",
    "        x = torch.cat([x, torch.tensor([[nxt]], device=device)], dim=1)\n",
    "    return x[0].tolist()\n",
    "\n",
    "def decode(tokens):\n",
    "    return \"\".join(itos[t] for t in tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9397b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train\n",
    "# -----------------------\n",
    "\n",
    "model = TinyGPT(V, d_model=64, n_heads=2, n_layers=2, max_len=4).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "\n",
    "steps = 500\n",
    "batch_size = 512\n",
    "\n",
    "for step in range(1, steps+1):\n",
    "    inp, tgt = make_batch(batch_size, device)\n",
    "    logits = model(inp)  # [B, 4, V]\n",
    "    loss = F.cross_entropy(logits.reshape(-1, V), tgt.reshape(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        # quick accuracy on the final answer position (predict last digit after '=')\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=-1)          # [B, 4]\n",
    "            acc = (pred == tgt).float().mean().item()\n",
    "        print(f\"step {step:4d} | loss {loss.item():.4f} | token-acc {acc*100:5.1f}%\")\n",
    "\n",
    "# Demo: generate answer digit from \"a+b=\"\n",
    "for a, b in [(3,7), (9,9), (2,5), (4,8)]:\n",
    "    prompt = [stoi[str(a)], stoi['+'], stoi[str(b)], stoi['=']]\n",
    "    out = sample(model, prompt, steps=1)  # predict one token (the answer digit)\n",
    "    print(decode(out), \" (expected:\", (a+b)%10, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = TinyGPT(V, d_model=64, n_heads=2, n_layers=2, max_len=4).to(device)\n",
    "prompt = '1+3='\n",
    "decode(sample(model, [stoi[c] for c in prompt], steps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8048e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "for i in range(4):\n",
    "    prompt = decode(sample(model, [stoi[c] for c in prompt], steps=1))\n",
    "    print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(p) for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise embedding using umap\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "emb = model.tok.weight.detach().cpu().numpy()  # [V, d_model]\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.1)\n",
    "emb_2d = reducer.fit_transform(emb)  # [V, 2]\n",
    "plt.scatter(emb_2d[:,0], emb_2d[:,1])\n",
    "for i, c in enumerate(itos):\n",
    "    plt.text(emb_2d[i,0], emb_2d[i,1], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb17eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(3,3), diagonal=1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74be82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
