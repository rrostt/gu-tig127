{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fe566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c560b",
   "metadata": {},
   "source": [
    "In this Colab, we will use DistilBERT, as it is a small model.\n",
    "It is distinct from the GPT models, in that it does not use Masked Multi-head attention.\n",
    "This means that each output is allowed to attend to future tokens.\n",
    "So do not be alarmed if you see that an output can attend future tokens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38489578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2e141",
   "metadata": {},
   "source": [
    "We will use two sentences where the word _bank_ is ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sentences with the same word \"bank\" but different meanings\n",
    "sent1 = \"The bank of the river was calm.\"\n",
    "sent2 = \"She went to the bank to deposit money.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a66c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences and get input IDs\n",
    "# So far bank, is represented with the same token.\n",
    "inputs = tokenizer([sent1, sent2], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# print token ids mapped to their respective tokens\n",
    "sent1_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "sent2_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][1])\n",
    "print(f\"Tokens for sent1: {sent1_tokens}\")\n",
    "print(f\"Tokens for sent2: {sent1_tokens}\")\n",
    "sent1_bank_index = sent1_tokens.index('bank')\n",
    "sent2_bank_index = sent2_tokens.index('bank')\n",
    "bank1 = inputs['input_ids'][0][sent1_bank_index].item()  # 'bank' in sent1\n",
    "bank2 = inputs['input_ids'][1][sent2_bank_index].item()  # 'bank' in sent2\n",
    "print(f\"Token ID for 'bank' in sent1: {bank1}\")\n",
    "print(f\"Token ID for 'bank' in sent2: {bank2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f80fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will pass the inputs through the model to get the hidden states\n",
    "# The hidden states are the outputs of each layer in the model.\n",
    "# By comparing the hidden states of the 'bank' token in both sentences, \n",
    "# we can see how the model differentiates between the two meanings based on context.\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bd8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can first look at the architecture of the model and the number of hidden layers\n",
    "# There is the embeddings layer, and 6 transformer layers in DistilBERT, so we should have 7 hidden states (including the input embeddings).\n",
    "print(f\"Number of hidden layers: {len(hidden_states)}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now look at the last hidden state (the output of the last transformer layer) for the 'bank' token in both sentences.\n",
    "# And we can compare the embeddings of the 'bank' token in both sentences to see how they differ based on context.\n",
    "output_layer = hidden_states[-1]\n",
    "bank1 = output_layer[0][sent1_bank_index]\n",
    "bank2 = output_layer[1][sent2_bank_index]\n",
    "# compare the two 'bank' token embeddings\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(bank1.unsqueeze(0), bank2.unsqueeze(0)).item()\n",
    "print(f\"Cosine similarity between 'bank' in sent1 and sent2: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f26311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this for each hidden layer to see how the embeddings evolve across layers\n",
    "# early layers should have more similar embeddings for 'bank' in both sentences, \n",
    "# while later layers should differentiate more based on context.\n",
    "for i, layer in enumerate(hidden_states):\n",
    "    bank1 = layer[0][sent1_bank_index]\n",
    "    bank2 = layer[1][sent2_bank_index]\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(bank1.unsqueeze(0), bank2.unsqueeze(0)).item()\n",
    "    print(f\"Layer {i}: Cosine similarity between 'bank' in sent1 and sent2: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that the model has learned to differentiate between the two meanings of 'bank' based on context, \n",
    "# as the cosine similarity is not very high, indicating that the embeddings for 'bank' in both sentences are different.\n",
    "#\n",
    "# Now we check the attention weights for the 'bank' token in both sentences to see how the model attends to different \n",
    "# parts of the input when processing the 'bank' token.\n",
    "\n",
    "inputs = tokenizer([sent1, sent2], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attentions = outputs.attentions  # This is a tuple of attention weights for each transformer layer\n",
    "# attentions is a tuple of length equal to the number of transformer layers, each element is a tensor \n",
    "# of shape (batch_size, num_heads, seq_length, seq_length)\n",
    "# so attentions[0] gives the attention for the first transformer layer, and attentions[0][0] gives the \n",
    "# attention weights for the first sentence in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fe797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualise the attention for the last layer, head 0 (the first attention head) for the 'bank' token in both sentences.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Get the attention weights for the last layer and head 0\n",
    "last_layer_attention_sent1 = attentions[-1][0]  # shape (num_heads, seq_length, seq_length)\n",
    "last_layer_attention_sent2 = attentions[-1][1]  # shape (num_heads, seq_length, seq_length)\n",
    "\n",
    "head0_attention_sent1 = last_layer_attention_sent1[8]  # shape (seq_length, seq_length)\n",
    "head0_attention_sent2 = last_layer_attention_sent2[8]  # shape (seq_length, seq_length)\n",
    "# Get the attention weights for the 'bank' token in both sentences\n",
    "bank1_attention = head0_attention_sent1[sent1_bank_index]  # attention weights for 'bank' in sent1\n",
    "bank2_attention = head0_attention_sent2[sent2_bank_index]\n",
    "\n",
    "# Visualise the attention weights for 'bank' in sent1\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.barplot(x=sent1_tokens,\n",
    "            y=bank1_attention.detach().numpy())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Where 'bank' attends in sent1\")\n",
    "plt.show()# Visualise the attention weights for 'bank' in sent2\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.barplot(x=sent2_tokens,\n",
    "            y=bank2_attention.detach().numpy())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Where 'bank' attends in sent2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b662952",
   "metadata": {},
   "source": [
    "Note that above, the word output for bank attend to Bank, as the end of sentence (period).\n",
    "\n",
    "Let's check the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ab53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over all heads in the first layer\n",
    "avg_attention_sent1 = attentions[0][0].mean(dim=0)  # shape (seq_length, seq_length)\n",
    "avg_attention_sent2 = attentions[0][1].mean(dim=0)  # shape (seq_length, seq_length)\n",
    "bank1_avg_attention = avg_attention_sent1[sent1_bank_index]  # shape (seq_length,)\n",
    "bank2_avg_attention = avg_attention_sent2[sent2_bank_index]  # shape (seq_length,)\n",
    "# Visualise the average attention weights for 'bank' in sent1\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.barplot(x=sent1_tokens,\n",
    "            y=bank1_avg_attention.detach().numpy())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average attention for 'bank' in sent1\")\n",
    "plt.show()\n",
    "# Visualise the average attention weights for 'bank' in sent2\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.barplot(x=sent2_tokens,\n",
    "            y=bank2_avg_attention.detach().numpy())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average attention for 'bank' in sent2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d98e32",
   "metadata": {},
   "source": [
    "Now we can see that in the first layer, the word bank in the first sentence attend to RIVER, whereas in the second sentence it attends to DEPOSIT and MONEY. These are the words that disambiguate what _bank_ actually means in the sentence. It means that the model has learned to understand context of words.\n",
    "\n",
    "We can look at the attention for the word Bank, over all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build layer Ã— token matrix (averaged over heads)\n",
    "def compute_layer_heatmap(attn_tensor, bank_index):\n",
    "    # attn_tensor shape: (layers, heads, seq_len, seq_len)\n",
    "    layer_maps = []\n",
    "    for layer in range(attn_tensor.shape[0]):\n",
    "        # Select layer\n",
    "        layer_attention = attn_tensor[layer]  # (heads, seq_len, seq_len)\n",
    "\n",
    "        # Select bank row across heads\n",
    "        bank_attention = layer_attention[:, bank_index, :]  # (heads, seq_len)\n",
    "\n",
    "        # Average across heads\n",
    "        mean_attention = bank_attention.mean(dim=0)  # (seq_len,)\n",
    "\n",
    "        layer_maps.append(mean_attention)\n",
    "\n",
    "    return torch.stack(layer_maps)  # (layers, seq_len)\n",
    "\n",
    "heatmap1 = compute_layer_heatmap(torch.stack(attentions)[:,0], sent1_bank_index)\n",
    "heatmap2 = compute_layer_heatmap(torch.stack(attentions)[:,1], sent2_bank_index)\n",
    "heatmap1 = heatmap1 / heatmap1.sum(dim=1, keepdim=True)\n",
    "heatmap2 = heatmap2 / heatmap2.sum(dim=1, keepdim=True)\n",
    "num_layers = heatmap1.shape[0]\n",
    "\n",
    "# Plot Sentence 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap1.detach().numpy(),\n",
    "            xticklabels=sent1_tokens,\n",
    "            yticklabels=[f\"L{i}\" for i in range(num_layers)],\n",
    "            cmap=\"viridis\")\n",
    "plt.title(\"Attention to Tokens from 'bank' Across Layers (Sentence 1)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Sentence 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap2.detach().numpy(),\n",
    "            xticklabels=sent2_tokens,\n",
    "            yticklabels=[f\"L{i}\" for i in range(num_layers)],\n",
    "            cmap=\"viridis\")\n",
    "plt.title(\"Attention to Tokens from 'bank' Across Layers (Sentence 2)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0f11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
