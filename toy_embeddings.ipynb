{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312dab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project with a mock vocabulary to show how word2vec learns.\n",
    "\n",
    "# mock text data using king queen prince princess, and france capital paris city. We will train a word2vec model to learn the relationships between these words.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from collections import Counter\n",
    "# Sample text data\n",
    "text = \"\"\"\n",
    "king queen king prince princess king queen prince princess\n",
    "king prince princess king queen prince princess\n",
    "france capital paris france capital paris france capital paris\n",
    "paris capital france paris capital france paris capital france\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "counter = Counter(words)\n",
    "vocab = sorted(counter.keys())\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "embedding_dim = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727563ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data preparation, creating pairs\n",
    "training_data = []\n",
    "window_size = 2\n",
    "for i in range(len(words)):\n",
    "    target_word = words[i]\n",
    "    context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(words), i + window_size + 1)))\n",
    "    for j in context_indices:\n",
    "        context_word = words[j]\n",
    "        training_data.append((word_to_index[target_word], word_to_index[context_word]))\n",
    "print(f\"Num training pairs: {len(training_data)}\")\n",
    "print(f\"Sample training pairs: {training_data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is composed of Wv and Wu, which are the input and output word embeddings. \n",
    "# The forward pass computes the dot product between the input word embedding and the output word embeddings of the context words, and applies a sigmoid function to get probabilities. \n",
    "# The loss is computed using negative sampling, where we sample negative examples from the vocabulary and compute the loss for both positive and negative examples.\n",
    "\n",
    "Wv = torch.randn(len(vocab), embedding_dim, requires_grad=True)\n",
    "Wu = torch.randn(len(vocab), embedding_dim, requires_grad=True)\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.5\n",
    "batch_size = 64\n",
    "num_negatives = 5\n",
    "\n",
    "def draw_embeddings(Wv, index_to_word):\n",
    "    import matplotlib.pyplot as plt\n",
    "    Wv_np = Wv.detach().numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(len(index_to_word)):\n",
    "        plt.scatter(Wv_np[i, 0], Wv_np[i, 1])\n",
    "        plt.text(Wv_np[i, 0] + 0.01, Wv_np[i, 1] + 0.01, index_to_word[i], fontsize=9)\n",
    "    plt.title(\"Word Embeddings\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = torch.randint(0, len(training_data), (batch_size,))\n",
    "\n",
    "    # collect v_c and u_c for the batch\n",
    "    v_c = Wv[[training_data[i][0] for i in idx]]\n",
    "    u_c = Wu[[training_data[i][1] for i in idx]]\n",
    "\n",
    "    # positive examples\n",
    "    pos_scores = torch.sum(v_c * u_c, dim=1)\n",
    "    pos_loss = -torch.log(torch.sigmoid(pos_scores)).mean()\n",
    "\n",
    "    # negative sampling\n",
    "    neg_indices = torch.randint(0, len(vocab), (batch_size, num_negatives))\n",
    "    neg_u_c = Wu[neg_indices]\n",
    "    neg_scores = torch.bmm(neg_u_c, v_c.unsqueeze(2)).squeeze()\n",
    "    neg_loss = -torch.log(torch.sigmoid(-neg_scores)).mean()\n",
    "\n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        Wv -= learning_rate * Wv.grad\n",
    "        Wu -= learning_rate * Wu.grad\n",
    "        Wv.grad.zero_()\n",
    "        Wu.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        embeddings.append(Wv.detach().clone())\n",
    "\n",
    "# draw_embeddings(Wv, index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_embeddings(embeddings[-1], index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e08024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Fix axis limits ONCE so they donâ€™t rescale\n",
    "all_emb = torch.stack(embeddings)\n",
    "xmin, xmax = all_emb[:,:,0].min(), all_emb[:,:,0].max()\n",
    "ymin, ymax = all_emb[:,:,1].min(), all_emb[:,:,1].max()\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    Wv_np = embeddings[frame].numpy()\n",
    "\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    ax.scatter(Wv_np[:, 0], Wv_np[:, 1])\n",
    "\n",
    "    for i in range(len(index_to_word)):\n",
    "        ax.text(Wv_np[i, 0], Wv_np[i, 1], index_to_word[i], fontsize=9)\n",
    "\n",
    "    ax.set_title(f\"Epoch {frame}\")\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update,\n",
    "    frames=len(embeddings),\n",
    "    interval=500,\n",
    "    blit=False\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5546a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
